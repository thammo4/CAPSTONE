{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd4333b1-420a-4118-873d-157ee421f6c1",
   "metadata": {},
   "source": [
    "# How to Use\n",
    "\n",
    "## Step 1\n",
    "Run the first cell to install a package that parses the captions. It is sometimes a bit funky on Rivanna but should work with a little trial.\n",
    "\n",
    "## Step 2\n",
    "Run the second cell and check the mini_test.csv just to make sure everything is running properly.\n",
    "\n",
    "## Step 3\n",
    "Add your first name to the csv title:\n",
    "`'firstname_1k_set_n.csv'`\n",
    "where n is the set number you are running.\n",
    "\n",
    "## Step 4\n",
    "Change the `'firstname'` value to your first name in all the cells. Then press run on all of them at once so they execute in a row as they complete and you can just leave it in the background.\n",
    "\n",
    "## Step 5\n",
    "Upload to GH in final dataset folder.\n",
    "\n",
    "## Notes on Data Decisions\n",
    "1. Some images don't have a wikicommons description (parsed caption). If this is the case, we return a `None` type, which can turn into an NA in your DF.\n",
    "2. Some WikiCommons image captions are written not in English. Most of these I've found to also not have a tag for the language they are written in. I'll keep looking into this, but this may just hurt our model.\n",
    "3. I'm working on fixing link funcitonality, but right now links output as `[anchor text](link)`. I want to get it to just output the anchor text."
   ]
  },
  {
   "cell_type": "raw",
   "id": "92d2550f-025d-4978-b24c-756366b60e83",
   "metadata": {
    "tags": []
   },
   "source": [
    "!sudo python -m pip install html2text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5c0e9d-6ba0-4c2d-933a-abc6b4b4a9ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import html2text\n",
    "\n",
    "def get_random_commons_ids(num_files=1):\n",
    "    api_url = 'https://commons.wikimedia.org/w/api.php'\n",
    "    api_params = {\n",
    "        'action': 'query',\n",
    "        'list': 'random',\n",
    "        'rnnamespace': 6,  # namespace=6 is for files\n",
    "        'rnlimit': num_files,\n",
    "        'format': 'json'\n",
    "    }\n",
    "\n",
    "    r = requests.get(url=api_url, params=api_params)\n",
    "    data = r.json()['query']['random']\n",
    "\n",
    "    wc_page_ids = [[wc_file['id'],wc_file['title']] for wc_file in data]\n",
    "    if num_files == 1:\n",
    "        wc_page_ids = wc_page_ids[0]\n",
    "\n",
    "    return wc_page_ids\n",
    "\n",
    "#\n",
    "# Function to map WikiMedia Commons uploads to their corresponding WikiData Entities\n",
    "#\n",
    "\n",
    "def get_commons_description(wc_title):\n",
    "    commons_api_url = f'https://commons.wikimedia.org/w/api.php?action=query&titles={wc_title}&prop=imageinfo&iiprop=extmetadata&format=json'\n",
    "    response = requests.get(commons_api_url)\n",
    "    data = response.json()\n",
    "    pages = data.get('query', {}).get('pages', {})\n",
    "    page_id = next(iter(pages))\n",
    "    metadata = pages[page_id].get('imageinfo', [{}])[0].get('extmetadata', {})\n",
    "    return metadata\n",
    "\n",
    "def get_q_number(wc_page_id, verbose=False):\n",
    "    # Construct the WikiBase Entity ID\n",
    "    wb_entity_id = f'M{wc_page_id}'\n",
    "\n",
    "    # Configure API Settings for GET Request to Commons API\n",
    "    api_url = 'https://commons.wikimedia.org/w/api.php'\n",
    "    api_params = {\n",
    "        'action': 'wbgetentities',\n",
    "        'ids': wb_entity_id,\n",
    "        'format': 'json',\n",
    "        'props': 'claims'\n",
    "    }\n",
    "\n",
    "    # Make GET Request to Commons API\n",
    "    response = requests.get(url=api_url, params=api_params)\n",
    "    data = response.json()\n",
    "\n",
    "    # Check if the entity exists and has statements\n",
    "    if 'entities' not in data or wb_entity_id not in data['entities'] or 'statements' not in data['entities'][wb_entity_id]:\n",
    "        return None\n",
    "\n",
    "    data_statements = data['entities'][wb_entity_id]['statements']\n",
    "\n",
    "    # Check if 'P180' (depicts) property exists\n",
    "    if 'P180' not in data_statements or not data_statements['P180']:\n",
    "        return None\n",
    "\n",
    "    # Assuming 'P180' exists and has at least one entry\n",
    "    depicts = data_statements['P180'];\n",
    "    \n",
    "    q_numbers = [];\n",
    "\n",
    "    # Extract relevant information pertaining to the `Depicts` statement\n",
    "    for x in range(len(depicts)):\n",
    "        depicts_data = depicts[x]['mainsnak']\n",
    "        # Check if 'datavalue' key exists in depicts_data\n",
    "        if depicts_data and 'datavalue' in depicts_data and 'value' in depicts_data['datavalue']:\n",
    "            wd_item = depicts_data['datavalue']['value']\n",
    "            wd_item_id = wd_item['id']\n",
    "            q_numbers.append(wd_item_id)\n",
    "        else:\n",
    "            return None\n",
    "    if verbose:\n",
    "        print('Collected')\n",
    "\n",
    "    return q_numbers\n",
    "\n",
    "def get_wikidata_label(wd_item_id):\n",
    "    api_url = 'https://www.wikidata.org/w/api.php'\n",
    "    api_params = {\n",
    "        'action': 'wbgetentities',\n",
    "        'ids': wd_item_id,\n",
    "        'format': 'json',\n",
    "        'props': 'labels',\n",
    "        'languages': 'en'\n",
    "    }\n",
    "\n",
    "    r = requests.get(api_url, api_params)\n",
    "    data = r.json()\n",
    "\n",
    "    if 'entities' in data and wd_item_id in data['entities'] and 'labels' in data['entities'][wd_item_id] and 'en' in data['entities'][wd_item_id]['labels']:\n",
    "        label = data['entities'][wd_item_id]['labels']['en']['value']\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    return label\n",
    "\n",
    "def get_wikidata_description(wd_item_id):\n",
    "    api_url = 'https://www.wikidata.org/w/api.php'\n",
    "    api_params = {\n",
    "        'action': 'wbgetentities',\n",
    "        'ids': wd_item_id,\n",
    "        'format': 'json',\n",
    "        'props': 'descriptions',\n",
    "        'languages': 'en'\n",
    "    }\n",
    "\n",
    "    r = requests.get(api_url, api_params)\n",
    "    data = r.json()\n",
    "\n",
    "    if 'entities' in data and wd_item_id in data['entities'] and 'descriptions' in data['entities'][wd_item_id] and 'en' in data['entities'][wd_item_id]['descriptions']:\n",
    "        description = data['entities'][wd_item_id]['descriptions']['en']['value']\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    return description\n",
    "\n",
    "#\n",
    "# Define function that will construct a convenient dataframe by repeatedly making calls to the wiki commons/data APIs\n",
    "# until a sufficiently large data set has been collected.\n",
    "# • Note that the number of calls ≥ number of rows in returned dataframe\n",
    "# • To create a CSV containing the returned dataframe:\n",
    "#       df = siki_wiki(25);\n",
    "#       df.to_csv('df_wiki.csv');\n",
    "#\n",
    "\n",
    "#this changes HTML link formats to just be the text of the link\n",
    "class MyHTML2Text(html2text.HTML2Text):\n",
    "    def handle_a(self, link, title, text):\n",
    "        # Return only the text of the link\n",
    "        return text\n",
    "\n",
    "# Create an instance of your custom HTML2Text class\n",
    "h = MyHTML2Text()\n",
    "\n",
    "def siki_wiki(row_count_wanted, row_print=25):\n",
    "    df_wiki = pd.DataFrame({'file_name': [], 'wiki_commons_id': [], 'wiki_data_id': [], 'depicts': [], 'description': [], 'parsed caption': []})\n",
    "\n",
    "    df_row_count = 0\n",
    "    enough_rows = False\n",
    "\n",
    "    while not enough_rows:\n",
    "        row = {'file_name': '', 'wiki_commons_id': '', 'wiki_data_id': '', 'depicts': '', 'description': '', 'parsed caption': ''}\n",
    "        wc_page_ids = get_random_commons_ids(100)\n",
    "\n",
    "        for wc_page_id in wc_page_ids:\n",
    "            wd_depicts_statements = []\n",
    "            #get depicts q numbers for the file\n",
    "            q_numbers = get_q_number(wc_page_id[0])\n",
    "            #get wikicommons description\n",
    "            wc_description = get_commons_description(wc_page_id[1])\n",
    "            #if there are depicts statements then we convert q numbers to depicts statements\n",
    "            if q_numbers:\n",
    "                wd_depicts_statements = [get_wikidata_label(x) for x in q_numbers]\n",
    "            #if there is a description, we add it\n",
    "            if wc_description:\n",
    "                    row['commons_description'] = wc_description\n",
    "            else:\n",
    "                    row['commons_description'] = 'NA'\n",
    "            #if there are depicts statements, then we get descriptions for those depicts vals\n",
    "            if wd_depicts_statements:\n",
    "                wd_descriptions = [get_wikidata_description(x) for x in q_numbers]\n",
    "                if wd_descriptions:\n",
    "                    row['file_name'] = wc_page_id[1]  # Assuming JPG format for simplicity\n",
    "                    row['wiki_commons_id'] = wc_page_id[0]\n",
    "                    row['wiki_data_id'] = q_numbers\n",
    "                    row['depicts'] = wd_depicts_statements\n",
    "                    row['description'] = wd_descriptions\n",
    "                    if 'ImageDescription' not in wc_description or 'value' not in wc_description['ImageDescription']:\n",
    "                        row['parsed caption'] = None\n",
    "                    else:\n",
    "                        row['parsed caption'] = h.handle(wc_description['ImageDescription']['value'])\n",
    "                    df_row = pd.DataFrame.from_dict(row, orient='index').T\n",
    "                    df_wiki = pd.concat([df_wiki, df_row], ignore_index=True)\n",
    "\n",
    "                    df_row_count = df_wiki.shape[0]\n",
    "                    if df_row_count%row_print==0:\n",
    "                        print(df_row_count)\n",
    "\n",
    "                    if df_row_count >= row_count_wanted:\n",
    "                        enough_rows = True\n",
    "                        break\n",
    "\n",
    "    return df_wiki\n",
    "\n",
    "# Example usage:\n",
    "df = siki_wiki(5,1) #the 1 just tells it to print every submission, the default is 25\n",
    "df.to_csv('mini_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f753aa3-f25f-4952-add7-b4f0a4554bcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = siki_wiki(1000)\n",
    "df.to_csv('Will_1k_set_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65c200d-eda9-4119-acc9-4bedf532f998",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = siki_wiki(1000)\n",
    "df.to_csv('Will_1k_set_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69b88c2-be40-409d-acc4-6a8aa1f3545f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = siki_wiki(1000)\n",
    "df.to_csv('Will_1k_set_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57a9534-a3d7-4726-8f04-d1c23a83d1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = siki_wiki(1000)\n",
    "df.to_csv('Will_1k_set_4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae61b079-928c-499b-97c3-18f26bf0e5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = siki_wiki(1000)\n",
    "df.to_csv('Will_1k_set_5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09acd58d-1dd7-4d65-8cab-96f4c7e6c464",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = siki_wiki(1000)\n",
    "df.to_csv('Will_1k_set_6.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
